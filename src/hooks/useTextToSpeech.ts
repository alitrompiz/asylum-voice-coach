import { useState, useRef, useCallback, useEffect } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useLanguagePreference } from './useLanguagePreference';

interface TTSOptions {
  voice?: string;
  onStart?: () => void;
  onEnd?: () => void;
  onError?: (error: Error) => void;
}

export const useTextToSpeech = () => {
  const [isPlaying, setIsPlaying] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [debugInfo, setDebugInfo] = useState<string>('');
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const currentRequestRef = useRef<string | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const { language, getVoiceForTTS, languageCode } = useLanguagePreference();

  // Detect iOS
  const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent) || 
    (navigator.platform === 'MacIntel' && navigator.maxTouchPoints > 1);

  // Audio recovery system for handling interruptions
  useEffect(() => {
    const handleVisibilityChange = () => {
      console.log('ğŸ‘ï¸ Visibility changed:', document.hidden ? 'hidden' : 'visible');
      if (!document.hidden && audioRef.current) {
        // Page became visible again, check if audio needs recovery
        setTimeout(() => {
          if (audioRef.current && audioRef.current.paused && isPlaying) {
            console.log('ğŸ”„ Attempting audio recovery after visibility change');
            audioRef.current.play().catch(err => {
              console.error('âŒ Audio recovery failed:', err);
            });
          }
        }, 100);
      }
    };

    const handleInterruption = () => {
      console.log('ğŸ“± Audio interruption detected');
      if (audioContextRef.current && audioContextRef.current.state === 'suspended') {
        console.log('ğŸ”„ Resuming AudioContext after interruption');
        audioContextRef.current.resume().catch(err => {
          console.error('âŒ AudioContext resume failed:', err);
        });
      }
    };

    // Listen for page visibility changes
    document.addEventListener('visibilitychange', handleVisibilityChange);
    
    // Listen for focus events (app coming back to foreground)
    window.addEventListener('focus', handleInterruption);
    window.addEventListener('pageshow', handleInterruption);
    
    // iOS specific interruption handling
    if (isIOS) {
      document.addEventListener('touchstart', handleInterruption, { once: true });
      document.addEventListener('click', handleInterruption, { once: true });
    }

    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange);
      window.removeEventListener('focus', handleInterruption);
      window.removeEventListener('pageshow', handleInterruption);
      if (isIOS) {
        document.removeEventListener('touchstart', handleInterruption);
        document.removeEventListener('click', handleInterruption);
      }
    };
  }, [isPlaying, isIOS]);

  const speak = useCallback(async (text: string, options: TTSOptions = {}) => {
    if (!text.trim()) {
      console.log('âŒ TTS: Empty text provided');
      return;
    }

    const requestId = `${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    currentRequestRef.current = requestId;
    
    console.log('ğŸ¤ TTS speak() called:', { 
      requestId,
      textLength: text.length, 
      languageCode,
      isIOS,
      textPreview: text.substring(0, 50) + '...',
      currentAudioElement: !!audioRef.current,
      isCurrentlyPlaying: isPlaying,
      isCurrentlyLoading: isLoading
    });

    // Stop any existing audio
    if (isPlaying || isLoading) {
      console.log('ğŸ›‘ Stopping existing TTS');
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current.currentTime = 0;
        audioRef.current = null;
      }
      setIsPlaying(false);
      setIsLoading(false);
    }

    try {
      setIsLoading(true);
      options.onStart?.();

      const voice = options.voice || getVoiceForTTS('elevenlabs');

      console.log('ğŸ“ Calling ElevenLabs TTS function:', { 
        requestId,
        textLength: text.length, 
        voice, 
        languageCode
      });

      const { data, error } = await supabase.functions.invoke('eleven-labs-tts-v2', {
        body: {
          text,
          voice,
          model: 'eleven_turbo_v2_5',
        },
      });
      
      console.log('ğŸ“¨ TTS API Response:', { 
        hasData: !!data, 
        hasError: !!error,
        dataKeys: data ? Object.keys(data) : null,
        requestId: data?.requestId || 'unknown'
      });

      if (currentRequestRef.current !== requestId) {
        console.log('ğŸš« TTS request cancelled');
        return;
      }

      if (error) {
        console.error('âŒ ElevenLabs TTS error:', error);
        throw new Error(error.message || 'TTS failed');
      }

      if (data?.audioContent) {
        console.log('âœ… TTS response received:', { 
          requestId,
          contentLength: data.audioContent.length
        });

        // Create audio element with special iOS handling
        console.log('ğŸ”Š Creating audio element for iOS');
        
        // For iOS, we need to use a different approach due to autoplay restrictions
        if (isIOS) {
          console.log('ğŸ Using iOS-compatible approach');
          
          try {
            // Create a blob URL instead of data URL for iOS compatibility
            const audioBytes = atob(data.audioContent);
            const arrayBuffer = new ArrayBuffer(audioBytes.length);
            const uint8Array = new Uint8Array(arrayBuffer);
            
            for (let i = 0; i < audioBytes.length; i++) {
              uint8Array[i] = audioBytes.charCodeAt(i);
            }
            
            // Log first few bytes to verify audio data integrity
            console.log('ğŸ” Audio data verification:', {
              firstBytes: Array.from(uint8Array.slice(0, 10)).map(b => b.toString(16)),
              byteLength: uint8Array.length
            });
            
            const blob = new Blob([arrayBuffer], { type: 'audio/mpeg' });
            const audioUrl = URL.createObjectURL(blob);
            
            // Initialize AudioContext FIRST before creating audio element
            const AudioContext = window.AudioContext || (window as any).webkitAudioContext;
            let audioContext: AudioContext | null = null;
            
            if (AudioContext) {
              audioContext = new AudioContext();
              audioContextRef.current = audioContext; // Store for recovery
              console.log('ğŸ”Š AudioContext created, state:', audioContext.state);
              
              // Resume audio context if suspended (critical for iOS)
              if (audioContext.state === 'suspended') {
                await audioContext.resume();
                console.log('ğŸ”Š AudioContext resumed, new state:', audioContext.state);
              }
            }
            
            // Create new audio element with full browser support
            const audio = new Audio();
            audio.src = audioUrl;
            audio.preload = 'auto';
            audio.volume = 1.0;
            audio.muted = false; // Ensure not muted
            
            // CRITICAL: Store the audio element IMMEDIATELY
            audioRef.current = audio;
            console.log('âœ… Audio element stored in ref');
            
            // Connect to AudioContext if available
            if (audioContext) {
              try {
                const source = audioContext.createMediaElementSource(audio);
                source.connect(audioContext.destination);
                console.log('ğŸ”Š Audio connected to AudioContext');
              } catch (contextError) {
                console.warn('âš ï¸ AudioContext connection failed:', contextError);
                // Continue without AudioContext connection
              }
            }

            // Set up event handlers
            const cleanup = () => {
              console.log('ğŸ§¹ Audio cleanup called');
              if (currentRequestRef.current === requestId) {
                setIsPlaying(false);
                audioRef.current = null;
                URL.revokeObjectURL(audioUrl); // Clean up blob URL
                options.onEnd?.();
              }
            };

            const handleError = (e: any) => {
              console.error('âŒ iOS Audio error:', e, audio.error);
              if (currentRequestRef.current === requestId) {
                setIsLoading(false);
                setIsPlaying(false);
                audioRef.current = null;
                URL.revokeObjectURL(audioUrl);
                options.onError?.(new Error(`iOS audio playback failed: ${e.message || audio.error?.message || 'Unknown error'}`));
              }
            };

            audio.addEventListener('ended', cleanup);
            audio.addEventListener('error', handleError);
            audio.addEventListener('loadstart', () => console.log('ğŸ“¥ Audio load started'));
            audio.addEventListener('canplay', () => console.log('âœ… Audio can play'));
            audio.addEventListener('canplaythrough', () => console.log('âœ… Audio can play through'));
            
            // Wait for the audio to be ready with a longer timeout for iOS
            console.log('â³ Waiting for audio to be ready...');
            await new Promise<void>((resolve, reject) => {
              const timeout = setTimeout(() => {
                reject(new Error('Audio load timeout after 10 seconds'));
              }, 10000);
              
              // Both canplaythrough and canplay events to maximize compatibility
              const readyHandler = () => {
                console.log('âœ… Audio ready handler called');
                clearTimeout(timeout);
                resolve();
              };
              
              audio.addEventListener('canplaythrough', readyHandler, { once: true });
              audio.addEventListener('canplay', readyHandler, { once: true });
              
              audio.addEventListener('error', (errorEvent) => {
                console.error('âŒ Audio load error:', errorEvent, audio.error);
                clearTimeout(timeout);
                reject(new Error(`Audio load failed: ${audio.error?.message || 'Unknown error'}`));
              }, { once: true });
              
              // Force load to start
              console.log('ğŸ”„ Starting audio load...');
              audio.load();
            });

            console.log('ğŸ Audio ready, attempting playback');
            
            // Try to play
            try {
              const playPromise = audio.play();
              await playPromise;
              console.log('âœ… iOS audio playback started successfully');
              
              if (currentRequestRef.current === requestId) {
                setIsLoading(false);
                setIsPlaying(true);
              }
            } catch (playError) {
              console.error('âŒ iOS play() failed:', playError);
              
              // iOS might require user interaction - set up a one-time touch handler
              if (playError.name === 'NotAllowedError') {
                console.log('âš ï¸ iOS requires user interaction, setting up touch handler');
                
                const playOnTouch = async () => {
                  try {
                    await audio.play();
                    console.log('âœ… Audio played after touch interaction');
                    setIsLoading(false);
                    setIsPlaying(true);
                  } catch (touchPlayError) {
                    console.error('âŒ Touch play failed:', touchPlayError);
                    handleError(touchPlayError);
                  }
                  document.removeEventListener('touchstart', playOnTouch);
                  document.removeEventListener('click', playOnTouch);
                };
                
                document.addEventListener('touchstart', playOnTouch, { once: true });
                document.addEventListener('click', playOnTouch, { once: true });
                
                // Set loading to false but don't set error
                setIsLoading(false);
                console.log('â³ Waiting for user interaction to play audio');
              } else {
                throw playError;
              }
            }
          } catch (iosError) {
            console.error('âŒ iOS audio setup failed:', iosError);
            throw new Error(`iOS audio setup failed: ${iosError.message}`);
          }
          
        } else {
          // Standard approach for non-iOS
          console.log('ğŸ–¥ï¸ Using standard audio approach');
          const audio = new Audio(`data:audio/mpeg;base64,${data.audioContent}`);
          audio.volume = 1.0;
          audio.muted = false;
          audio.preload = 'auto';
          
          audioRef.current = audio;

          const cleanup = () => {
            if (currentRequestRef.current === requestId) {
              setIsPlaying(false);
              audioRef.current = null;
              options.onEnd?.();
            }
          };

          const handleError = (e: any) => {
            console.error('âŒ Audio error:', e);
            if (currentRequestRef.current === requestId) {
              setIsLoading(false);
              setIsPlaying(false);
              audioRef.current = null;
              options.onError?.(new Error('Audio playback failed'));
            }
          };

          audio.addEventListener('ended', cleanup);
          audio.addEventListener('error', handleError);

          await audio.play();
          console.log('âœ… Standard audio playback started');
          
          if (currentRequestRef.current === requestId) {
            setIsLoading(false);
            setIsPlaying(true);
          }
        }

        // Verify playback after 100ms
        setTimeout(() => {
          if (audioRef.current && !audioRef.current.paused) {
            console.log('ğŸ“Š Audio confirmed playing:', {
              currentTime: audioRef.current.currentTime,
              duration: audioRef.current.duration,
              paused: audioRef.current.paused
            });
          } else {
            console.warn('âš ï¸ Audio may not be playing');
          }
        }, 100);

      } else {
        throw new Error('No audio content received');
      }

    } catch (error) {
      console.error('âŒ TTS error:', error);
      if (currentRequestRef.current === requestId) {
        setIsLoading(false);
        setIsPlaying(false);
        options.onError?.(error as Error);
      }
    }
  }, [language, getVoiceForTTS, languageCode, isPlaying, isLoading, isIOS]);

  const stop = useCallback(() => {
    console.log('ğŸ›‘ TTS stop() called');
    currentRequestRef.current = null;
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
      audioRef.current = null;
    }
    setIsPlaying(false);
    setIsLoading(false);
  }, []);
  
  // Enhanced debug function to inspect audio state and diagnose interruptions
  const debugAudio = useCallback(() => {
    const debugMessages: string[] = [];
    
    // Current state info
    debugMessages.push(`ğŸµ TTS State: playing=${isPlaying}, loading=${isLoading}`);
    debugMessages.push(`ğŸ“± Platform: ${isIOS ? 'iOS' : 'non-iOS'}`);
    debugMessages.push(`ğŸ‘ï¸ Page visible: ${!document.hidden}`);
    debugMessages.push(`ğŸ”Š Page focused: ${document.hasFocus()}`);
    
    // Check if we have an audio element
    if (!audioRef.current) {
      debugMessages.push('âš ï¸ No audio element available');
    } else {
      // Check audio element properties
      const audio = audioRef.current;
      debugMessages.push(`ğŸ“Š Audio state: ${audio.paused ? 'paused' : 'playing'}`);
      debugMessages.push(`ğŸ“Š Current time: ${audio.currentTime.toFixed(2)}s`);
      debugMessages.push(`ğŸ“Š Duration: ${audio.duration ? audio.duration.toFixed(2) + 's' : 'unknown'}`);
      debugMessages.push(`ğŸ“Š Volume: ${audio.volume}`);
      debugMessages.push(`ğŸ“Š Muted: ${audio.muted}`);
      debugMessages.push(`ğŸ“Š Ready state: ${audio.readyState}`);
      debugMessages.push(`ğŸ“Š Network state: ${audio.networkState}`);
      debugMessages.push(`ğŸ“Š Error: ${audio.error ? audio.error.message : 'none'}`);
      
      // Try to force play and report result immediately
      audio.play().then(() => {
        console.log('âœ… Forced play successful');
        debugMessages.push('âœ… Forced play successful');
        setDebugInfo(debugMessages.join('\n'));
      }).catch(err => {
        console.log('âŒ Forced play failed:', err.message);
        debugMessages.push(`âŒ Forced play failed: ${err.message}`);
        setDebugInfo(debugMessages.join('\n'));
      });
    }
    
    // Check AudioContext state
    if (audioContextRef.current) {
      debugMessages.push(`ğŸ”Š AudioContext state: ${audioContextRef.current.state}`);
      debugMessages.push(`ğŸ”Š AudioContext sample rate: ${audioContextRef.current.sampleRate}`);
    } else {
      debugMessages.push('âš ï¸ No AudioContext available');
    }
    
    // Check for common interruption scenarios
    const AudioContext = window.AudioContext || (window as any).webkitAudioContext;
    if (!AudioContext) {
      debugMessages.push('âš ï¸ AudioContext not supported');
    } else {
      debugMessages.push('âœ… AudioContext supported');
      try {
        const testCtx = new AudioContext();
        debugMessages.push(`ğŸ“Š New AudioContext state: ${testCtx.state}`);
        testCtx.close(); // Clean up test context
      } catch (e) {
        debugMessages.push(`âŒ AudioContext creation failed: ${e.message}`);
      }
    }
    
    // Browser compatibility checks
    debugMessages.push(`ğŸŒ User Agent: ${navigator.userAgent.substring(0, 60)}...`);
    debugMessages.push(`ğŸ”Š Audio support: ${!!window.Audio}`);
    debugMessages.push(`ğŸµ MediaDevices: ${!!navigator.mediaDevices}`);
    
    setDebugInfo(debugMessages.join('\n'));
    return debugMessages.join('\n');
  }, [isIOS, isPlaying, isLoading]);

  return {
    speak,
    stop,
    isPlaying,
    isLoading,
    debugAudio,
    debugInfo,
    currentLanguage: language,
  };
};